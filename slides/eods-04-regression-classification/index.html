<!DOCTYPE html>
<html>
  <head>
    <title>Regression and Classification</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
    </style>
    <link rel="stylesheet" href="../style.css">
  </head>
  <body>
    <textarea id="source">

class: center, middle

Elements of Data Science - F2019

# Regression and Classification

09/30/2019

---
Summary

- Correlation
- Simple Linear Regression
- Multiple Regression
- Interpreting Coefficents
- Regression for Real Valued Prediction
- Regression for Classification
- Distance Based Models
- Tree Based Models
- Neural Network Models

---
# Example Tips Data

- Load tips dataset

.smaller[
```python
tips = sns.load_dataset('tips')
tips.shape
```
```
(244, 7)
```]
--
count:false

.smaller[
```python
tips.head(5)
```]
![:scale 50%](images/tips_head.png)

---
# Correlation

- **Question:** are total_bill and tips correlated?

--
count:false
.smallest[
```python
sns.jointplot(x='total_bill',y='tip',data=tips);
```]
.center[
![](images/tips_totalbill_joint.png)]

---
# Aside: Pearson Corr Coef.

- **Question:** are total_bill and tips correlated? 
--
count:false
- Could calculate Pearson Correlation Coefficient
--
count:false
- Assumes normally distributed data! (which is not true here)

--
count:false
```python
from scipy.stats import pearsonr

r,p = pearsonr(tips.total_bill,tips.tip)
print(f'r: {r:.2f}, p: {p:.2f}')
```
```
r: 0.68, p: 0.00
```


<br>
--
count:false
.smallest[
[On the Effects of Non-Normality on the Distribution of the Sample Product-Moment Correlation Coefficient](https://www.jstor.org/stable/2346598?seq=1#page_scan_tab_contents)
]

---
# Obligitory Correlation vs. Causation

.center[![](images/correlation.png)]

--
count:false
- correlation does not mean causation!

--
count:false
- causal inference
--
count:false
    - controlled experiment
--
count:false
    - control for confounding variables


---
# Spurious Correlation

--
count:false
- Also, look hard enough and you'll find correlation.
    - See [spurious correlations](https://www.tylervigen.com/spurious-correlations) for examples
--
count:false
![](images/spurious_correlation.png)



---
#Linear Relationship

- Is there a linear relationship between total_bill and tips?

--
count:false
.smallest[
```python
sns.jointplot(x='total_bill',y='tip',data=tips, kind='reg');
```]
.center[
![](images/tips_totalbill_joint_reg.png)]
]


---
#Simple Linear Regression

<br>
--
count:false
.center[
$\Large y_i = \beta x_i + \alpha + \varepsilon_i$ 
]


<br>
--
count:false
- **$y_i$** : dependent, endogenous, target, label (Ex: `tips`)

--
count:false
- **$x_i$** : independent, exogenous , feature, attribute (Ex: `total_bill`)

--
count:false
- **$\beta$** : coefficient, slope

--
count:false
- **$\alpha$** : bias term, intercept

--
count:false
- **$\varepsilon_i$** : error, hopefully small, often assumed $\mathcal{N}(0,1)$


--
count:false
- Want to find values for $\beta$ and $\alpha$ that best fit the data.


---
#Finding $\beta$ and $\alpha$

--
count:false
- **prediction**: $\hat{y}_i = f(x_i) = \beta x_i + \alpha$

--
count:false

- **error**: $error(y_i,\hat{y}_i) = y_i - \hat{y}_i$

--
count:false

- **sum of squared errors**: $\sum_{i=1:n}{\left(y_i - \hat{y}_i\right)^2}$


--
count:false
- **least squares**: make the sum of squared errors as small as possible


--
count:false
- **gradient descent**: minimize error by following the gradient wrt $\beta,\alpha$
--
count:false
    - can sometime be optimized in closed form
--
count:false
    - often done iteratively

---
#Aside: Gradient Descent

--
count:false
- Want to maximize or minimize something (Ex: squared error)

--
count:false
- **Gradient** : direction, vector of partial derivatives
--
count:false
    - can get complicated, will often estimate this

--
count:false
- **Gradient Descent** : take steps wrt the direction of the gradient 
--
count:false
    - **maximize** : in the direction of the gradient
--
count:false
    - **minimize** : in the opposite direction of the gradient

--
count:false
- **Global Maximum/Minimum** : the single best solution

--
count:false
- **Local Maximum/Minimum** : the best solution in the neighborhood


---
#Aside: Gradient Descent

.center[
![](images/gradient_descent.png)]

- Finding a global min using gradient descent

.smallest[
- From Data Science From Scratch ([Chapter 8](https://ezproxy.cul.columbia.edu/login?qurl=https%3a%2f%2fsearch.ebscohost.com%2flogin.aspx%3fdirect%3dtrue%26db%3dnlebk%26AN%3d979529%26site%3dehost-live%26scope%3dsite&ebv=EK&ppid=Page-__-84))]

---
# OLS in Statsmodels

--
count:false
- OLS : Ordinary Least Squares

--
count:false
.smaller[
```python
import statsmodels.api as sm

X = tips['total_bill']     # independent variable
X = sm.add_constant(X)     # bias term

y = tips['tip']            # dependent variable

model_slr = sm.OLS(y,X)    # initialize the model
result = model_slr.fit()   # find the best fit
```]

--
count:false
.smaller[
```python
result.params
```]
.smaller[
```
const         0.920270  # alpha
total_bill    0.105025  # beta
dtype: float64
```]

--
count:false
.smallest[
[ols documentation](https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html)
]

---
# Interpreting Coefficients

.smaller[
```
const         0.920270  # alpha
total_bill    0.105025  # beta
dtype: float64
```]


--
count:false
.center[
tips = 0.11 * total_bill + 0.92]

<br>
--
count:false
- What are tips when total_bill = 0?
--
count:false
    - tips = .92 (tips start at about $1)


--
count:false
- How do changes in total_bill affect tip?
--
count:false
    - when we increase total_bill by 1
--
count:false
    - tips go up 11 cents


---
# Plotting the Fit

```python
y_hat = result.predict(np.array([[1,0],[1,50]]))

ax = sns.scatterplot(tips.total_bill,tips.tip);
ax.plot([0,50],y_hat);
```
.center[
![:scale 50%](images/fit_plot.png)]


---
# Evaluating Fit: Residuals

--
count:false
- Residuals : $y - \hat{y}$
- We we like to see normally distributed error

.smallest[
```python
x_obs = np.random.rand(500)                              # our observed x values
y_obs = 1*x_obs + 0 + np.random.normal(0,1.0,size=(500)) # observed y, with error
y_hat = 1*x_obs + 0                                      # a very good guess
resid_ = y_obs - y_hat                                   # residuals
ax = sns.scatterplot(y_hat,resid_);
ax.set_xlabel('y_hat'); ax.set_ylabel('residual');
```]
.center[![](images/residplot_normal.png)]

---
# Evaluating Fit: Residuals

```python
ax = sns.scatterplot(X.iloc[:,1],result.resid/np.std(result.resid))
ax.set_ylabel('standardized residual');
```
.center[![:scale 50%](images/tips_resid.png)]

[Interpreting residual plots](http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/)

---
# Evaluating Fit: R-Squared

- **R-Squared** or **Coefficient of Determination** :
.smaller[
    - **Fraction of the total variation** in the **dependent variable** captured by **the model**]



--
count:false
- $\bar{y} = \frac{1}{n}\sum\_{i=1}^n y\_i$


--
count:false
- $SS\_{tot} = \frac{1}{n}\sum\_{i}\left(y_i - \bar{y}\right)^2$



--
count:false
- $SS\_{res} = \frac{1}{n}\sum\_{i}\left(y_i - \hat{y}\_i\right)^2$


--
count:false
- $R^2 = 1 - \frac{SS\_{res}}{SS_{tot}}$


--
count:false
- Max: 1, all variation captured

--
count:false
- Min: ?

--
count:false
```python
model_slr.rsquared # 0.457
```

---
# Multiple Linear Regression

--
count:false
- Including multiple independent variables

--
count:false
.center[
$y\_i = \beta\_0 + \beta\_1 x\_{i1} + \beta\_2 x\_{i2} + \ldots + \beta\_m x\_{im} + \varepsilon\_i$]

Note: $\beta_0 \equiv \alpha$



--
count:false
- Ex: 
.center[
`tips = beta_0 + beta_1 total_bill + beta_2 size`
]

--
count:false
.smaller[
```python
X = tips[['total_bill','size']]
X = sm.add_constant(X)

y = tips['tip']

model_mlr = sm.OLS(y,X).fit()
```]


--
count:false
- Note: 'multivarariate' usually refers to multiple *dependent* variabels

---
# MLR: Interpreting Coefficients 

```python
model_mlr.params
```
```
const         0.668945
total_bill    0.092713
size          0.192598
dtype: float64
```

--
count:false
- If we hold everything else constant, what effect does the variable have

--
count:false
- If `size` is held constant, a rise of 1 total_bill -> rise of .09 tip

--
count:false
- If `total_bill` is held constant, a rise of 1 size -> rise of .19 tip

--
count:false
- Can add interaction terms to allow both to move
    - Ex: total_bill * size
    - more complicated to interpret

---
# Colinarity

--
count:false
- MLS assumes features are linearly independent
--
count:false
- Can't rewrite one column as a weighted sum of the others
--
count
- Ex: `entrees ordered` will likely be linearly related to `size`


--
count:false
- Issue: Model won't know how to estimate $\beta$
    - If we add to one and subtract from the other, there will be no change

--
count:false
- Try to remove obvious colinearity
    - can use correlation and linear regression to detect

--
count:false
- Important to consider when constructing categorical features

---
# MLR: R-Squared

--
count:false
- $R^2 = 1 - \frac{\frac{1}{n}\sum\_{i}\left(y\_i - \hat{y}\_i\right)^2}{\frac{1}{n}\sum\_{i}\left(y_i - \bar{y}\right)^2}$


--
count:false
- an increase in the number of features will only increase $R^2$

--
count:false
- Adjusted $R^2$: account for the number of features

--
count:false

$R\_{adj}^2 = 1 - (1-R^2)\frac{n - 1}{n-m-1}$

- $n$ is number of observations, $m$ the number of features


--
count:false
.smaller[
```python
model_mlr.rsquared, model_mlr.rsquared_adj
```]
.smaller[
```
0.468, 0.463
```]

---
#Statsmodels Summary
.smallest[
```python
model_mlr.summary()
```]
.center.smallest[```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                    tip   R-squared:                       0.468
Model:                            OLS   Adj. R-squared:                  0.463
Method:                 Least Squares   F-statistic:                     105.9
Date:                Mon, 30 Sep 2019   Prob (F-statistic):           9.67e-34
Time:                        11:16:52   Log-Likelihood:                -347.99
No. Observations:                 244   AIC:                             702.0
Df Residuals:                     241   BIC:                             712.5
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.6689      0.194      3.455      0.001       0.288       1.050
total_bill     0.0927      0.009     10.172      0.000       0.075       0.111
size           0.1926      0.085      2.258      0.025       0.025       0.361
==============================================================================
Omnibus:                       24.753   Durbin-Watson:                   2.100
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               46.169
Skew:                           0.545   Prob(JB):                     9.43e-11
Kurtosis:                       4.831   Cond. No.                         67.6
==============================================================================
```]

---
# Aside: Interpretation Vs. Prediction

--
count:false
- Interpretation: Explain how observed features relate to observed target

--
count:false
- Prediction: Given new features, can we generate a prediction


--
count:false
- Often asked to do one or the other, be clear which is most important


--
count:false
- In prediction, may not worry about interpreting the model!

--
count:false
- There is a push to change this, increase interpretability

---
class:middle

# Questions re Regression?

---
# Classification

--
count:false
- **Regression** -> predict a real value (Ex. predict tip)

--
count:false
- **Classification** -> predict a discrete class, category


--
count:false
- **Binary** classification : two categories 
    - pos/neg, cat/dog, win/lose
--
count:false
- **Multiclass** classification : more than two categories 
    - red/green/blue, flower type, integer 0-10

--
count:false
- **Multilabel** classification : can assign more than one label to an instance
    - paper topics, entities in image


---
# Tips as Classification

--
count:false
Instead of tip amount, let's look at high or low tips.

--
count:false
.smaller[
```python
def map_tips(x):
    return True if x > tips.tip.median() else False

tips['tip_high'] = tips.tip.apply(map_tips)
```]

--
count:false
.center[![](images/tips_lowhigh.png)]

---
# Tips as Classification
.center[![](images/tips_lowhigh_slr.png)]

--
count:false
- want a number between 0 and 1
--
count:false
- want something that looks like a threshold

---
# Logistic Regression

- $logistic(x) = \frac{1}{1+e^{(-x)}}$

--
count:false
.smallest[
```python
def logistic(x):
    return 1 / (1+np.exp(-x))

x = np.linspace(-10,10,1000)
plt.plot(x,logistic(x));
plt.xlabel('x');plt.ylabel('logistic(x)');
```]

.center[![](images/logistic.png)]

---
# Logistic Regression with sklearn

Our problem (with one feature) becomes:

.center[$y_i = logistic(\beta_0 + \beta_1 x_i) + \varepsilon_i$]


--
count:false
```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(fit_intercept=True)
lr.fit(tips.total_bill.values.reshape(-1,1),tips.tip_high)
print(f'alpha = {lr.intercept_[0]:0.2f}')
print(f'beta_1 = {lr.coef_[0][0]:0.2f}')
```
```
beta_0 = -2.96
beta_1 = 0.16
```

---
# Interpreting Logistic Regression

--
count:false

- After some math

.center[
$\log\left(\frac{y\_i}{1-y\_i}\right) = \beta\_0 + \beta\_1 x\_{i1}$]

--
count:false
- log odds ratio of p(y=1)/p(y=0)

--
count:false

- odds range from 0 to positive infinity

--
count:false
- odds(5) -> 5/1 -> 5 out of 6 times -> .83

--
count:false
- odds(.2) -> 1/5 -> 1 out of 6 times -> .16

--
count:false
See [here](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/) for a good explanation


---
# Review

--
count:false
- Regression
--
count:false
    - simple linear regression
--
count:false
    - multiple linear regression
--
count:false
- Classification
--
count:false
    - logistic regression


--
count:false
- So far all **Linear Models**

--
count:false
- Next up:
--
count:false
    - distance based
--
count:false
    - tree based
--
count:false
    - network based





---
class:middle

# Questions?

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);

    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
