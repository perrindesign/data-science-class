{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Quiz\n",
    "\n",
    "## Perrin Anto - paj2117\n",
    "\n",
    "### Due Sat. Nov. 23, 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this quiz, we're going to load documents from 2 topics (space, cars) in the 20newsgroups dataset.\n",
    "\n",
    "The goal is to train a classifier that classifies documents into these 2 topics based on a term frequency representation of the documents.\n",
    "\n",
    "We will then calculate mean cross-validaion accuracy of a RandomForestClassifier using this transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1187"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import fetch_20newsgroups from sklearn.datasets\n",
    "from sklearn.datasets import fetch_20newsgroups \n",
    "\n",
    "# Load the dataset into newsgroups using fetch_20newsgroups.\n",
    "#   Only fetch the training subset using subset='train'.\n",
    "#   Only fetch the two topics using categories=['sci.space','rec.autos']\n",
    "# Store in the result into newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset='train',categories=['sci.space','rec.autos'])\n",
    "\n",
    "# Store the newsgroups.data as docs, newsgroups.target as y and newsgroups.target_names as y_names\n",
    "docs = newsgroups.data\n",
    "y = newsgroups.target\n",
    "y_names = newsgroups.target_names\n",
    "\n",
    "# Print the number of observations by printing the length of docs\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: prb@access.digex.com (Pat)\\nSubject: Re: Proton/Centaur?\\nOrganization: Express Access Online Communications USA\\nLines: 15\\nNNTP-Posting-Host: access.digex.net\\n\\n\\nWell thank you dennis for your as usual highly detailed and informative \\nposting.   \\n\\nThe question i have about the proton, is  could it be  handled at\\none of KSC's spare pads, without major  malfunction,  or could it be\\nhandled at kourou  or Vandenberg?   \\n\\nNow if it uses storables,  then  how long would it take for the russians\\nto equip something at cape york?\\n\\nIf  Proton were launched from a western site,  how would it compare to the\\nT4/centaur?   As i see it, it should lift  very close to the T4.\\n\\npat\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the text of the first document in docs.\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the target value of the first document in y.\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sci.space'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the target_name of the first document using y_names and y.\n",
    "y_names[y[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CountVectorizer to Convert To TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1187, 3628)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import CountVectorizer from sklearn.feature_extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize a CountVectorizer object. It should\n",
    "#   lowercase all text, \n",
    "#   include both unigrams and bigrams\n",
    "#   exclude terms that occur in fewer than 10 documents\n",
    "#   exclude terms that occur in more that 95% of documents\n",
    "#   exclude all 'english' stopwords\n",
    "# Store as cvect\n",
    "cvect = CountVectorizer(lowercase=True,\n",
    "                     ngram_range=(1,2),\n",
    "                     min_df=10,\n",
    "                     max_df=.95,\n",
    "                     stop_words='english')\n",
    "\n",
    "# Fit cvect on docs and transform docs into their term frequency representation.\n",
    "# Store as X_tf\n",
    "X_tf = cvect.fit_transform(docs)\n",
    "\n",
    "# Print the shape of X_tf. \n",
    "# The number of rows should match the number of documents \n",
    "#    and the number of columns should be in the thousands\n",
    "X_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lobby people', 'come familiar', '4101 email', '80 aslv', 'anybody concerned']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The stopwords learned by cvect are stored as a set in cvect.stop_words_\n",
    "# We'd like to print out a small subset of these terms.\n",
    "# One way to get a subset of a set is to treat it as a list.\n",
    "# First, convert the stop_words_ set to a list.\n",
    "# Store as stop_words_list\n",
    "stop_words_list = list(cvect.stop_words_)\n",
    "\n",
    "# Print out the first 5 elements in stop_words_list.\n",
    "# Note that, since a set is unordered, \n",
    "#    there is no meaning to the ordering of these terms and they may vary over runs.\n",
    "stop_words_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mean CV Accuracy Using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean cv scores: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Import cross_val_score from sklearn.model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Import RandomForestClassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Get a set of 5-fold CV scores using\n",
    "#    a RandomForestClassifier with 50 trees, X_tf and y\n",
    "# Store as cv_scores\n",
    "cv_scores = cross_val_score(RandomForestClassifier(n_estimators=50),X_tf,y,cv=5)\n",
    "\n",
    "# Print the mean of these cv_scores. The mean accuracy should be above .9\n",
    "print(f'mean cv scores: {np.mean(cv_scores):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Find Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer stores the feature names (terms in the vocabulary) in two ways:\n",
    "#  1. as a dictionary of term:colum_index pairs, accessed via cvect.vocabulary_\n",
    "#  2. as a list of terms, in column index order, accessed via cvect.get_feature_names()\n",
    "#\n",
    "# We can get the indices of the most important features by retraining a RandomForestClassifier on X,y\n",
    "#  and accessing .feature_importances_\n",
    "#\n",
    "# Using some combination of the above data-structures, \n",
    "#  print out the top 10 terms in the vocabulary\n",
    "#  ranked by the feature importances learned by a RandomForestClassifier\n",
    "# \n",
    "# The terms you find will likely not be surprising given the document categories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods-f19",
   "language": "python",
   "name": "eods-f19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
