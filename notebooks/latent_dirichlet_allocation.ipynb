{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling using Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prerequisites:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Natural Language Processing Fundamentals in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Things to be familiar with: \n",
    "    - tokenization\n",
    "    - stopwords\n",
    "    - term frequency\n",
    "    - Bag-Of-Words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Going to discuss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is topic modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How does Latent Dirichlet Allocation (LDA) work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How to train and use LDA with gensim?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is topic modeling? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **topic**: a collection of related words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- a document can be composed of several topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Given a collection of documents, we can ask:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What words make up each topic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What topics make up each document?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://deliveryimages.acm.org/10.1145/2140000/2133826/figs/f1.jpg\">\n",
    "\n",
    "David Blei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### First, a simple example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What if we knew everything about our corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # we'll want this later\n",
    "\n",
    "vocab = ['baseball','cat','dog','pet','played','tennis']\n",
    "\n",
    "V = len(vocab) # size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "K = 2 # number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# the probability of each term given topic 1\n",
    "topic_1 = [.33,   0,   0,   0, .33, .33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# the probability of each term given topic 2\n",
    "topic_2 = [  0, .25, .25, .25, .25,   0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# per topic word distributions\n",
    "phi = [topic_1, topic_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(phi).shape) # K x V (number of topics x size of vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### If we had some documents, what topics make up each document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corpus = ['the dog and cat played tennis',\n",
    "          'tennis and baseball are sports',\n",
    "          'a dog or a cat can be a pet']\n",
    "\n",
    "# recall\n",
    "vocab = ['baseball','cat','dog','pet','played','tennis']\n",
    "\n",
    "phi = [[.33,   0,   0,   0, .33, .33],\n",
    "       [  0, .25, .25, .25, .25,   0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# per document topic distributions\n",
    "theta = [[.50, .50],\n",
    "         [.99, .01],\n",
    "         [.01, .99]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(theta).shape) # M x K (number of documents x number of topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We can even generate a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123) # for demo purposes\n",
    "\n",
    "N = 6 # number of tokens in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "new_theta = [.6,.4] # draw a topic distribution (theta die)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "new_doc = []\n",
    "for i in range(N):\n",
    "    z = np.argmax(np.random.multinomial(1, new_theta)) # get a topic\n",
    "    \n",
    "    idx = np.argmax(np.random.multinomial(1,phi[z]))   \n",
    "    x = vocab[idx]                                     # get a term\n",
    "    \n",
    "    new_doc.append(x)                                  # add to document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pet baseball pet tennis played played'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(new_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NOTE: But usually, we don't know the theta or phi!  \n",
    "### We need to learn these from a set of documents (corpus)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Uses for $\\phi$ (phi), the per topic word distributions:\n",
    "\n",
    "- infering labels for topics\n",
    "- word clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Uses for $\\theta$ (theta), the per document topic weights:\n",
    "\n",
    "- dimentionality reduction\n",
    "- clustering\n",
    "- similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we learn phi ($\\phi$) and theta ($\\theta$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    " - generative statistical model\n",
    " - *Blei, D., Ng, A., Jordan, M. Latent Dirichlet allocation. J. Mach. Learn. Res. 3 (Jan 2003)*\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dirichlet Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Conjugate prior to the Multinomial Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Multinomial is like a \"die\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Dirichlet is like a \"die factory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4d/Smoothed_LDA.png\" style=\"width: 30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "K     # number of topics\n",
    "\n",
    "phi   # per topic word distributions\n",
    "\n",
    "beta  # parameters for word distribution die factory, length = V\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "M     # number of documents\n",
    "N     # number of words/tokens in each document\n",
    "\n",
    "theta # per document topic distributions\n",
    "\n",
    "alpha # parameters for topic die factory, length = K\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "z     # topic indexes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "Dirichlet   # dirichlet distribution (aka die factory)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4d/Smoothed_LDA.png\" style=\"width: 30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "phi = []  # word distribution die, 1 per topic\n",
    "\n",
    "# pseudocode to generate topic word distributions\n",
    "for k in range(K):\n",
    "    phi.append(Dirichlet(beta,V).get_die())  # generate word distribution die\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "corpus = []\n",
    "\n",
    "# pseudocode to generate corpus\n",
    "for m in range(M):\n",
    "    document_m = []\n",
    "    \n",
    "    theta_m = Dirichlet(alpha,K).get_die()   # generate a topic die\n",
    "    \n",
    "    for n in range(N):\n",
    "        z_mn = theta_m.get_topic()     # roll topic die\n",
    "        w_mn = phi[z_mn].get_word()    # roll word distribution die\n",
    "        \n",
    "        document_m.append(w_mn)\n",
    "    \n",
    "    corpus.append(document_m)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Things we know: \n",
    "\n",
    " - M : the number of documents\n",
    " - N : the lengths of document\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Things we choose:\n",
    "\n",
    " - K : the number of topics\n",
    " - V : our vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Things we want to learn: \n",
    "\n",
    " - $\\theta$'s (theta's) : the per document topic weights\n",
    " - $\\phi$'s (phi's) : the per topic word weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Note:\n",
    "\n",
    "We may want to infer $\\alpha$ and $\\beta$ as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings # to deal with deprecation warnings\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups()\n",
    "X = newsgroups.data\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: jcm@head-cfa.harvard.edu (Jonathan McDowell) Subject: Re: Shuttle Launch Question Organization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA Distribution: sci Lines: 23  From artic'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example document\n",
    "X[4].replace('\\n',' ')[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=50, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 4175)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform our documents (this might take a moment)\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# this is our vocabulary (the column names of our dataset)\n",
    "feature_names = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '01', '02', '03', '04', '05', '06', '07', '08']\n",
      "['ysu', 'za', 'zealand', 'zero', 'zeus', 'zip', 'zone', 'zoo', 'zuma', 'zx']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[:10])\n",
    "print(feature_names[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning) # to remove warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create model with 20 topics\n",
    "lda = LatentDirichletAllocation(n_components=20,  # the number of topics\n",
    "                                n_jobs=-1,        # use all cpus\n",
    "                                random_state=123) # for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# learn phi and theta (lda.components_ and X_lda)\n",
    "# this will take a while!\n",
    "X_lda = lda.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00684573, 0.00684573, 0.00684573, 0.00684573, 0.00684573,\n",
       "       0.00684573, 0.00684573, 0.00684573, 0.00684573, 0.00684573,\n",
       "       0.00684573, 0.00684573, 0.00684573, 0.00684573, 0.00684573,\n",
       "       0.86993113, 0.00684573, 0.00684573, 0.00684573, 0.00684573])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lda[100] # lda representation of document_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 16, 12])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(X_lda[100])[::-1][:3] # the top topics of document_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# a utility function to print out the most likely terms for each topic\n",
    "# taken from https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic {:#2d}: \".format(topic_idx)\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0: msg food duke sensitivity chinese\n",
      "Topic  1: edu com writes article subject\n",
      "Topic  2: informatik font uni fonts tu\n",
      "Topic  3: ground amp audio circuit voltage\n",
      "Topic  4: henry space alaska cramer ti\n",
      "Topic  5: stratus wpi sw cdt atf\n",
      "Topic  6: god jesus bible christian christians\n",
      "Topic  7: umn navy mil tc minnesota\n",
      "Topic  8: israel israeli arab arabs jews\n",
      "Topic  9: usc lehigh eisa danny fraser\n",
      "Topic 10: cleveland cwru freenet edu ins\n",
      "Topic 11: mit rpi max lcs harris\n",
      "Topic 12: window motif widget eos server\n",
      "Topic 13: valley chuck routine verse daily\n",
      "Topic 14: key clipper encryption chip keys\n",
      "Topic 15: edu windows card thanks com\n",
      "Topic 16: edu columbia cmu cunixb ysu\n",
      "Topic 17: umich arbor ann rit isc\n",
      "Topic 18: digex access pat buffalo express\n",
      "Topic 19: __ ___ ai uga uci\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda,feature_names,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example using gensim"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups()\n",
    "corpus_fname = '../../../scikit_learn_data/20news-bydate_py3.data.txt'\n",
    "with open(corpus_fname,'w') as f:\n",
    "    for doc in newsgroups.data[:1000]:\n",
    "        f.write(doc.replace('\\n',' ') + '\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "newsgroups.data[4].replace('\\n',' ')[:200]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "from gensim.corpora import TextCorpus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "%time corpus = TextCorpus(input=corpus_fname)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "corpus.length # M"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "len(corpus.dictionary) # V"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "%%time \n",
    "\n",
    "K = 20\n",
    "\n",
    "lda = LdaModel(corpus=corpus,\n",
    "               id2word=corpus.dictionary,\n",
    "               num_topics=K,\n",
    "               passes=2, chunksize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What words make up each topic?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "lda.show_topic(15) # phi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p = lda.show_topic(15, topn=24635)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sum([k for j,k in x])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lda.show_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What topics make up each document?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "text = next(corpus.sample_texts(1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "lda[corpus.dictionary.doc2bow(text)] # theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topics covered:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is topic modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How does Latent Dirichlet Allocation (LDA) work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How to train and use LDA with sklearn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How to train and use LDA with gensim?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thank you!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "eods-f19",
   "language": "python",
   "name": "eods-f19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
